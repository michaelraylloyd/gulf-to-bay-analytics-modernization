{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef7b810",
   "metadata": {},
   "source": [
    "# Bronze Ingestion — Sales Analytics\n",
    "\n",
    "This notebook ingests raw CSV files from the Lakehouse Files area and writes\n",
    "Delta tables into the Bronze layer. Bronze is intentionally light‑touch:\n",
    "no business logic, no conformance, no joins. The goal is to land raw‑but‑readable\n",
    "Delta tables for downstream Silver and Gold transformations.\n",
    "\n",
    "### Source Files\n",
    "- `customers.csv`\n",
    "- `products.csv`\n",
    "- `sales.csv`\n",
    "\n",
    "### Workflow\n",
    "1. Read raw CSVs from Lakehouse Files  \n",
    "2. Apply minimal Bronze cleanup (trim whitespace)  \n",
    "3. Write Delta tables into the Lakehouse as `bronze_*`  \n",
    "4. Validate row counts  \n",
    "\n",
    "### Notes\n",
    "- This notebook reflects the chosen ingestion pattern after evaluating\n",
    "  Dataflow Gen2, Warehouse SQL, Pipelines, and Notebooks.\n",
    "- Notebooks provide a modern, Fabric‑native ingestion surface using Python and Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3024072-2358-4ad8-a79c-e5ec8af04ad8",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     StructType, StructField,\n\u001b[32m     12\u001b[39m     StringType, IntegerType, DoubleType\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 0. CLEAN BRONZE TABLES (DROP IF EXISTS)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mspark\u001b[49m.sql(\u001b[33m\"\u001b[39m\u001b[33mDROP TABLE IF EXISTS bronze_customers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m spark.sql(\u001b[33m\"\u001b[39m\u001b[33mDROP TABLE IF EXISTS bronze_products\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m spark.sql(\u001b[33m\"\u001b[39m\u001b[33mDROP TABLE IF EXISTS bronze_sales\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BRONZE INGESTION — SALES ANALYTICS (EXPLICIT SCHEMA VERSION)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "lakehouse_path = \"Files/\"\n",
    "\n",
    "files = {\n",
    "    \"customers\": \"customers.csv\",\n",
    "    \"products\":  \"products.csv\",\n",
    "    \"sales\":     \"sales.csv\"\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. DEFINE SCHEMAS\n",
    "# ------------------------------------------------------------\n",
    "schema_customers = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_products = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "schema_sales = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True)  # We'll cast this later in Silver\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. LOAD RAW CSV FILES (NO HEADER INFERENCE)\n",
    "# ------------------------------------------------------------\n",
    "df_customers = spark.read.csv(f\"{lakehouse_path}{files['customers']}\", header=False, schema=schema_customers)\n",
    "df_products  = spark.read.csv(f\"{lakehouse_path}{files['products']}\",  header=False, schema=schema_products)\n",
    "df_sales     = spark.read.csv(f\"{lakehouse_path}{files['sales']}\",     header=False, schema=schema_sales)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. BRONZE CLEANUP (TRIM STRINGS)\n",
    "# ------------------------------------------------------------\n",
    "def bronze_trim(df):\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype == \"string\":\n",
    "            df = df.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "    return df\n",
    "\n",
    "df_customers = bronze_trim(df_customers)\n",
    "df_products  = bronze_trim(df_products)\n",
    "df_sales     = bronze_trim(df_sales)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. WRITE BRONZE DELTA TABLES\n",
    "# ------------------------------------------------------------\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_customers\")\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_products\")\n",
    "df_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_sales\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. VALIDATION — ROW COUNTS\n",
    "# ------------------------------------------------------------\n",
    "print(\"Bronze table row counts:\")\n",
    "spark.sql(\"SELECT 'bronze_customers' AS table_name, COUNT(*) AS row_count FROM bronze_customers\").show()\n",
    "spark.sql(\"SELECT 'bronze_products'  AS table_name, COUNT(*) AS row_count FROM bronze_products\").show()\n",
    "spark.sql(\"SELECT 'bronze_sales'     AS table_name, COUNT(*) AS row_count FROM bronze_sales\").show()\n",
    "\n",
    "# ============================================================\n",
    "# END OF BRONZE INGESTION\n",
    "# ============================================================"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "217c25c8-e01f-4c1e-8dee-6198f70e8b3e",
    "default_lakehouse_name": "sales_analytics_lakehouse",
    "default_lakehouse_workspace_id": "d87dfcca-dff7-477e-ae07-42996ceb937a",
    "known_lakehouses": [
     {
      "id": "217c25c8-e01f-4c1e-8dee-6198f70e8b3e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
