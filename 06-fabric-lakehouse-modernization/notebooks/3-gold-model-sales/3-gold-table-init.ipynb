{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BLOCK 1 — GOLD TABLE SCHEMA METADATA\n",
    "# -------------------------------------------------------------------\n",
    "# Single source of truth for all Gold-layer table definitions.\n",
    "# Each key is a table name, and each value is an ordered mapping\n",
    "# of column names to Spark SQL data types.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "tables = {\n",
    "    \"dim_product\": {\n",
    "        \"product_sk\": \"bigint\",\n",
    "        \"product_id\": \"string\",\n",
    "        \"product_name\": \"string\",\n",
    "        \"product_number\": \"string\",\n",
    "        \"color\": \"string\",\n",
    "        \"standard_cost\": \"float\",\n",
    "        \"list_price\": \"float\",\n",
    "        \"size\": \"string\",\n",
    "        \"weight\": \"float\",\n",
    "        \"category\": \"string\",\n",
    "        \"subcategory\": \"string\"\n",
    "    },\n",
    "    \"fact_sales\": {\n",
    "        \"sales_sk\": \"bigint\",\n",
    "        \"order_id\": \"string\",\n",
    "        \"order_date\": \"string\",\n",
    "        \"customer_sk\": \"bigint\",\n",
    "        \"product_sk\": \"bigint\",\n",
    "        \"quantity\": \"int\",\n",
    "        \"unit_price\": \"float\",\n",
    "        \"discount\": \"float\",\n",
    "        \"line_total\": \"float\"\n",
    "    },\n",
    "    \"exceptions_fact_sales\": {\n",
    "        \"order_id\": \"string\",\n",
    "        \"order_date\": \"string\",\n",
    "        \"customer_id\": \"string\",\n",
    "        \"product_id\": \"string\",\n",
    "        \"quantity\": \"int\",\n",
    "        \"unit_price\": \"float\",\n",
    "        \"discount\": \"float\",\n",
    "        \"line_total\": \"float\",\n",
    "        \"reason\": \"string\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cde8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BLOCK 2 — GOLD TABLE GENERATOR (DROP + CREATE)\n",
    "# -------------------------------------------------------------------\n",
    "# Kill-and-fill pattern:\n",
    "# - DROP TABLE IF EXISTS <table>\n",
    "# - CREATE TABLE <table> (...) from metadata\n",
    "# Guarantees deterministic schema and wipes drift.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for table_name, columns in tables.items():\n",
    "\n",
    "    # Drop existing table if present\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "    # Build column definitions from metadata\n",
    "    col_defs = \",\\n    \".join([f\"{col} {dtype}\" for col, dtype in columns.items()])\n",
    "\n",
    "    # Create the table using Spark SQL types\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            {col_defs}\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BLOCK 3 — CONFIRMATION OUTPUT\n",
    "# -------------------------------------------------------------------\n",
    "# Simple console confirmation for notebook and pipeline runs.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for table_name in tables.keys():\n",
    "    print(f\"[INIT] Created table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda45b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BLOCK 4 — MAINTENANCE (OPTIONAL OPTIMIZE + ZORDER)\n",
    "# -------------------------------------------------------------------\n",
    "# Lightweight performance maintenance for Gold tables.\n",
    "# - OPTIMIZE compacts small files.\n",
    "# - ZORDER improves data skipping on common filter columns.\n",
    "#\n",
    "# NOTE:\n",
    "# - Adjust ZORDER columns per table as needed.\n",
    "# - Safe to no-op if OPTIMIZE/ZORDER not supported in environment.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "zorder_columns = {\n",
    "    \"dim_product\": [\"product_id\", \"category\"],\n",
    "    \"fact_sales\": [\"order_date\", \"customer_sk\", \"product_sk\"],\n",
    "    \"exceptions_fact_sales\": [\"order_date\", \"customer_id\", \"product_id\"]\n",
    "}\n",
    "\n",
    "for table_name in tables.keys():\n",
    "    try:\n",
    "        # Basic OPTIMIZE\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "\n",
    "        # Optional ZORDER if configured\n",
    "        if table_name in zorder_columns and zorder_columns[table_name]:\n",
    "            cols = \", \".join(zorder_columns[table_name])\n",
    "            spark.sql(f\"OPTIMIZE {table_name} ZORDER BY ({cols})\")\n",
    "\n",
    "        print(f\"[MAINT] Optimized table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Non-fatal: log and continue\n",
    "        print(f\"[MAINT][WARN] Maintenance skipped for {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd47c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BLOCK 5 — SCHEMA VALIDATION\n",
    "# -------------------------------------------------------------------\n",
    "# Validates that the physical table schema matches the metadata:\n",
    "# - Same columns\n",
    "# - Same data types (case-insensitive)\n",
    "# - Same column order\n",
    "#\n",
    "# Any mismatch is printed as a warning. You can choose to:\n",
    "# - raise an Exception to fail the pipeline, or\n",
    "# - log and continue.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def get_table_schema(table_name: str) -> StructType:\n",
    "    return spark.table(table_name).schema\n",
    "\n",
    "def normalize_dtype(dtype_str: str) -> str:\n",
    "    return dtype_str.strip().lower()\n",
    "\n",
    "validation_errors = []\n",
    "\n",
    "for table_name, columns in tables.items():\n",
    "    try:\n",
    "        schema = get_table_schema(table_name)\n",
    "        physical_cols = [(f.name, normalize_dtype(f.dataType.simpleString())) for f in schema]\n",
    "        expected_cols = [(col, normalize_dtype(dtype)) for col, dtype in columns.items()]\n",
    "\n",
    "        if physical_cols != expected_cols:\n",
    "            validation_errors.append((table_name, physical_cols, expected_cols))\n",
    "            print(f\"[VALIDATION][ERROR] Schema mismatch for {table_name}\")\n",
    "        else:\n",
    "            print(f\"[VALIDATION] Schema OK for {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        validation_errors.append((table_name, str(e), None))\n",
    "        print(f\"[VALIDATION][ERROR] Could not validate {table_name}: {e}\")\n",
    "\n",
    "# Optional: fail hard if any validation errors\n",
    "if validation_errors:\n",
    "    print(\"[VALIDATION][SUMMARY] One or more tables failed schema validation.\")\n",
    "    # Uncomment to fail the notebook/pipeline:\n",
    "    # raise Exception(\"Gold table schema validation failed. See logs above.\")\n",
    "else:\n",
    "    print(\"[VALIDATION][SUMMARY] All Gold tables passed schema validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1defa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# BLOCK 6 — LOGGING (GOLD INIT LOG TABLE)\n",
    "# -------------------------------------------------------------------\n",
    "# Writes a simple log entry to a Gold log table:\n",
    "# - run_timestamp\n",
    "# - status\n",
    "# - table_count\n",
    "# - validation_error_count\n",
    "#\n",
    "# This can be extended later with:\n",
    "# - pipeline_run_id\n",
    "# - user\n",
    "# - environment\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "log_table = \"gold_pipeline_log\"\n",
    "\n",
    "# Ensure log table exists (idempotent)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "        run_timestamp string,\n",
    "        process       string,\n",
    "        status        string,\n",
    "        table_count   int,\n",
    "        validation_error_count int\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "run_timestamp = datetime.utcnow().isoformat()\n",
    "process_name = \"gold_table_init\"\n",
    "status = \"SUCCESS\" if not validation_errors else \"FAILED\"\n",
    "table_count = len(tables)\n",
    "validation_error_count = len(validation_errors)\n",
    "\n",
    "log_df = spark.createDataFrame(\n",
    "    [\n",
    "        (run_timestamp, process_name, status, table_count, validation_error_count)\n",
    "    ],\n",
    "    [\"run_timestamp\", \"process\", \"status\", \"table_count\", \"validation_error_count\"]\n",
    ")\n",
    "\n",
    "log_df.write.mode(\"append\").insertInto(log_table)\n",
    "\n",
    "print(f\"[LOG] Wrote Gold init log entry: status={status}, tables={table_count}, validation_errors={validation_error_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
