{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee82f8e3",
   "metadata": {},
   "source": [
    "# Bronze Ingestion — Sales Table\n",
    "\n",
    "This notebook ingests raw sales data into the Bronze layer of the Lakehouse. It applies schema alignment, enforces deterministic column types, and writes the normalized dataset into the Bronze Delta table. The workflow is consolidated into a single execution block to maintain clarity, reproducibility, and operational consistency across environments.\n",
    "\n",
    "This ingestion pattern serves as the foundation for downstream Data Quality checks, SCD2 processing, and Silver transformations within the Lakehouse Expansion pillar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49060771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Read raw source data\n",
    "raw_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"/lakehouse/default/Files/raw/sales\")\n",
    ")\n",
    "\n",
    "# Step 2 — Apply schema normalization\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "bronze_df = (\n",
    "    raw_df\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(\"string\"))\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "    .withColumn(\"order_amount\", col(\"order_amount\").cast(\"double\"))\n",
    "    .withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    ")\n",
    "\n",
    "# Step 3 — Write to Bronze table\n",
    "bronze_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouse.bronze_sales\")\n",
    "\n",
    "# Step 4 — Return preview\n",
    "bronze_df.limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
