{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d856a4ee",
   "metadata": {},
   "source": [
    "# Silver Layer — Standardization and Type Enforcement\n",
    "\n",
    "The Silver layer transforms raw Bronze data into clean, typed, and standardized tables.\n",
    "\n",
    "It:\n",
    "\n",
    "- Trims all string fields\n",
    "- Enforces numeric types on selected columns\n",
    "- Routes failed casts into Silver exception tables\n",
    "- Preserves row-level fidelity for debugging and junior-friendly querying\n",
    "\n",
    "This notebook uses a deterministic **TRUNCATE-based kill-and-fill** pattern:\n",
    "\n",
    "- Tables persist permanently\n",
    "- Each run clears them with `TRUNCATE TABLE`\n",
    "- Clean rows append into Silver tables\n",
    "- Bad type conversions append into exception tables\n",
    "\n",
    "## Pipeline steps\n",
    "\n",
    "1. Ensure Silver and Silver exception tables exist (schema from Bronze).\n",
    "2. `TRUNCATE` all Silver and exception tables.\n",
    "3. Load Bronze tables.\n",
    "4. Trim all string columns.\n",
    "5. Enforce numeric types using a safe, lineage-stable cast helper.\n",
    "6. Route failed casts into exception tables.\n",
    "7. Append clean rows into Silver tables.\n",
    "8. Append exception rows into exception tables.\n",
    "9. Print row counts for quick validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Silver Transformations — Kill and Fill Using TRUNCATE\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import col, trim\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Ensure Silver + exception tables exist (schema from Bronze)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver_customers\n",
    "    USING DELTA AS\n",
    "    SELECT * FROM bronze_customers WHERE 1 = 0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver_products\n",
    "    USING DELTA AS\n",
    "    SELECT * FROM bronze_products WHERE 1 = 0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver_sales\n",
    "    USING DELTA AS\n",
    "    SELECT * FROM bronze_sales WHERE 1 = 0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_silver_customers\n",
    "    USING DELTA AS\n",
    "    SELECT * FROM silver_customers WHERE 1 = 0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_silver_products\n",
    "    USING DELTA AS\n",
    "    SELECT * FROM silver_products WHERE 1 = 0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_silver_sales\n",
    "    USING DELTA AS\n",
    "    SELECT * FROM silver_sales WHERE 1 = 0\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. TRUNCATE Silver + exception tables\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for tbl in [\n",
    "    \"silver_customers\", \"exceptions_silver_customers\",\n",
    "    \"silver_products\",  \"exceptions_silver_products\",\n",
    "    \"silver_sales\",     \"exceptions_silver_sales\"\n",
    "]:\n",
    "    spark.sql(f\"TRUNCATE TABLE {tbl}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Load Bronze\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "bronze_customers = spark.table(\"bronze_customers\")\n",
    "bronze_products  = spark.table(\"bronze_products\")\n",
    "bronze_sales     = spark.table(\"bronze_sales\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def clean_strings(df):\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, trim(col(c)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def enforce_numeric(df, colname, dtype):\n",
    "    \"\"\"\n",
    "    Casts a column to a numeric type using a temporary column to avoid\n",
    "    Spark analyzer lineage collisions.\n",
    "    Returns:\n",
    "      cleaned_df  - with column cast to dtype\n",
    "      exceptions  - rows where cast failed (same schema as df)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Cast into a temporary column\n",
    "    temp_col = f\"{colname}__cast\"\n",
    "    df2 = df.withColumn(temp_col, col(colname).cast(dtype))\n",
    "\n",
    "    # 2. Identify failed casts\n",
    "    failed = df2.filter(\n",
    "        col(colname).isNotNull() & col(temp_col).isNull()\n",
    "    ).select(df.columns)\n",
    "\n",
    "    # 3. Keep only successful rows\n",
    "    cleaned = df2.filter(\n",
    "        col(temp_col).isNotNull() | col(colname).isNull()\n",
    "    ).drop(temp_col)\n",
    "\n",
    "    # 4. Replace original column with casted version\n",
    "    cleaned = cleaned.withColumn(colname, col(colname).cast(dtype))\n",
    "\n",
    "    return cleaned, failed\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Clean customers — trim only\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "silver_customers_df = clean_strings(bronze_customers)\n",
    "exceptions_silver_customers_df = spark.createDataFrame([], silver_customers_df.schema)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Clean products — trim + enforce numeric\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "products = clean_strings(bronze_products)\n",
    "\n",
    "products, ex_p1 = enforce_numeric(products, \"standard_cost\", FloatType())\n",
    "products, ex_p2 = enforce_numeric(products, \"list_price\",   FloatType())\n",
    "products, ex_p3 = enforce_numeric(products, \"weight\",       FloatType())\n",
    "\n",
    "silver_products_df = products\n",
    "exceptions_silver_products_df = ex_p1.unionByName(ex_p2).unionByName(ex_p3)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Clean sales — trim + enforce numeric\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "sales = clean_strings(bronze_sales)\n",
    "\n",
    "sales, ex_s1 = enforce_numeric(sales, \"quantity\",   IntegerType())\n",
    "sales, ex_s2 = enforce_numeric(sales, \"unit_price\", FloatType())\n",
    "sales, ex_s3 = enforce_numeric(sales, \"discount\",   FloatType())\n",
    "sales, ex_s4 = enforce_numeric(sales, \"line_total\", FloatType())\n",
    "\n",
    "silver_sales_df = sales\n",
    "exceptions_silver_sales_df = (\n",
    "    ex_s1.unionByName(ex_s2)\n",
    "         .unionByName(ex_s3)\n",
    "         .unionByName(ex_s4)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. Append clean + exception rows\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "silver_customers_df.write.insertInto(\"silver_customers\")\n",
    "silver_products_df.write.insertInto(\"silver_products\")\n",
    "silver_sales_df.write.insertInto(\"silver_sales\")\n",
    "\n",
    "exceptions_silver_customers_df.write.insertInto(\"exceptions_silver_customers\")\n",
    "exceptions_silver_products_df.write.insertInto(\"exceptions_silver_products\")\n",
    "exceptions_silver_sales_df.write.insertInto(\"exceptions_silver_sales\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9. Validate counts\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for tbl in [\n",
    "    \"silver_customers\", \"exceptions_silver_customers\",\n",
    "    \"silver_products\",  \"exceptions_silver_products\",\n",
    "    \"silver_sales\",     \"exceptions_silver_sales\"\n",
    "]:\n",
    "    print(tbl, spark.table(tbl).count())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
