{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b90d94",
   "metadata": {},
   "source": [
    "# Gold Layer — Dimensional Modeling (Star Schema)\n",
    "\n",
    "The Gold layer transforms clean Silver data into a dimensional star schema optimized for analytics, BI, and semantic modeling.\n",
    "\n",
    "This layer:\n",
    "\n",
    "- Builds conformed dimensions (Customer, Product)\n",
    "- Builds a clean FactSales table\n",
    "- Enforces join integrity\n",
    "- Routes join failures into exception tables\n",
    "- Uses surrogate keys (optional, enabled here)\n",
    "- Uses a deterministic TRUNCATE-based kill-and-fill pattern\n",
    "\n",
    "## Pipeline steps\n",
    "\n",
    "1. Ensure Gold and Gold exception tables exist.\n",
    "2. `TRUNCATE` all Gold and exception tables.\n",
    "3. Load Silver tables.\n",
    "4. Build dimensions with surrogate keys.\n",
    "5. Build FactSales with validated foreign keys.\n",
    "6. Route join failures into exception tables.\n",
    "7. Append clean rows into Gold tables.\n",
    "8. Append exception rows into exception tables.\n",
    "9. Print row counts for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Gold Modeling — Star Schema Build (Kill and Fill)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Ensure Gold + exception tables exist\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_customer (\n",
    "          customer_sk     bigint\n",
    "        , customer_id     string\n",
    "        , first_name      string\n",
    "        , last_name       string\n",
    "        , address1        string\n",
    "        , address2        string\n",
    "        , city            string\n",
    "        , state_province  string\n",
    "        , country         string\n",
    "        , postal_code     string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_product (\n",
    "          product_sk      bigint\n",
    "        , product_id      string\n",
    "        , product_name    string\n",
    "        , product_number  string\n",
    "        , color           string\n",
    "        , standard_cost   float\n",
    "        , list_price      float\n",
    "        , size            string\n",
    "        , weight          float\n",
    "        , category        string\n",
    "        , subcategory     string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fact_sales (\n",
    "          sales_sk        bigint\n",
    "        , order_id        string\n",
    "        , order_date      string\n",
    "        , customer_sk     bigint\n",
    "        , product_sk      bigint\n",
    "        , quantity        int\n",
    "        , unit_price      float\n",
    "        , discount        float\n",
    "        , line_total      float\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Exception tables\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_fact_sales (\n",
    "          order_id        string\n",
    "        , order_date      string\n",
    "        , customer_id     string\n",
    "        , product_id      string\n",
    "        , quantity        int\n",
    "        , unit_price      float\n",
    "        , discount        float\n",
    "        , line_total      float\n",
    "        , reason          string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. TRUNCATE Gold + exception tables\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for tbl in [\n",
    "    \"dim_customer\",\n",
    "    \"dim_product\",\n",
    "    \"fact_sales\",\n",
    "    \"exceptions_fact_sales\"\n",
    "]:\n",
    "    spark.sql(f\"TRUNCATE TABLE {tbl}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Load Silver\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "silver_customers = spark.table(\"silver_customers\")\n",
    "silver_products  = spark.table(\"silver_products\")\n",
    "silver_sales     = spark.table(\"silver_sales\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Build Dimensions (with surrogate keys)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "dim_customer_df = (\n",
    "    silver_customers\n",
    "        .withColumn(\"customer_sk\", monotonically_increasing_id())\n",
    "        .select(\n",
    "            \"customer_sk\",\n",
    "            \"customer_id\",\n",
    "            \"first_name\",\n",
    "            \"last_name\",\n",
    "            \"address1\",\n",
    "            \"address2\",\n",
    "            \"city\",\n",
    "            \"state_province\",\n",
    "            \"country\",\n",
    "            \"postal_code\"\n",
    "        )\n",
    ")\n",
    "\n",
    "dim_product_df = (\n",
    "    silver_products\n",
    "        .withColumn(\"product_sk\", monotonically_increasing_id())\n",
    "        .select(\n",
    "            \"product_sk\",\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"product_number\",\n",
    "            \"color\",\n",
    "            \"standard_cost\",\n",
    "            \"list_price\",\n",
    "            \"size\",\n",
    "            \"weight\",\n",
    "            \"category\",\n",
    "            \"subcategory\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Build FactSales with validated foreign keys\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Join to dimensions\n",
    "fact_joined = (\n",
    "    silver_sales.alias(\"s\")\n",
    "        .join(dim_customer_df.alias(\"c\"), col(\"s.customer_id\") == col(\"c.customer_id\"), \"left\")\n",
    "        .join(dim_product_df.alias(\"p\"), col(\"s.product_id\") == col(\"p.product_id\"), \"left\")\n",
    ")\n",
    "\n",
    "# Identify join failures\n",
    "exceptions_fact_sales_df = fact_joined.filter(\n",
    "    col(\"c.customer_sk\").isNull() | col(\"p.product_sk\").isNull()\n",
    ").select(\n",
    "    \"s.order_id\",\n",
    "    \"s.order_date\",\n",
    "    \"s.customer_id\",\n",
    "    \"s.product_id\",\n",
    "    \"s.quantity\",\n",
    "    \"s.unit_price\",\n",
    "    \"s.discount\",\n",
    "    \"s.line_total\",\n",
    "    (\n",
    "        col(\"c.customer_sk\").isNull().cast(\"string\")\n",
    "        .alias(\"reason\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keep only successful rows\n",
    "fact_sales_df = fact_joined.filter(\n",
    "    col(\"c.customer_sk\").isNotNull() & col(\"p.product_sk\").isNotNull()\n",
    ").withColumn(\n",
    "    \"sales_sk\", monotonically_increasing_id()\n",
    ").select(\n",
    "    \"sales_sk\",\n",
    "    \"s.order_id\",\n",
    "    \"s.order_date\",\n",
    "    \"c.customer_sk\",\n",
    "    \"p.product_sk\",\n",
    "    \"s.quantity\",\n",
    "    \"s.unit_price\",\n",
    "    \"s.discount\",\n",
    "    \"s.line_total\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Append clean + exception rows\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "dim_customer_df.write.insertInto(\"dim_customer\")\n",
    "dim_product_df.write.insertInto(\"dim_product\")\n",
    "fact_sales_df.write.insertInto(\"fact_sales\")\n",
    "exceptions_fact_sales_df.write.insertInto(\"exceptions_fact_sales\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Validate counts\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for tbl in [\n",
    "    \"dim_customer\",\n",
    "    \"dim_product\",\n",
    "    \"fact_sales\",\n",
    "    \"exceptions_fact_sales\"\n",
    "]:\n",
    "    print(tbl, spark.table(tbl).count())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
