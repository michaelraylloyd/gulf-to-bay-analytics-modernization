{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef7b810",
   "metadata": {},
   "source": [
    "# Bronze layer — truncate + exception logging\n",
    "\n",
    "The Bronze layer ingests raw CSV files exactly as delivered, preserving source fidelity while capturing malformed rows into exception tables.\n",
    "\n",
    "This notebook uses a deterministic **TRUNCATE-based kill-and-fill** pattern:\n",
    "\n",
    "- Tables persist permanently\n",
    "- Each run clears them with `TRUNCATE TABLE`\n",
    "- Clean rows append into Bronze tables\n",
    "- Malformed rows append into exception tables\n",
    "- Schemas are explicitly defined\n",
    "- Exception tables include `_corrupt_record` for traceability\n",
    "- Logic is Fabric/Delta–native and transferable to other modern stacks\n",
    "\n",
    "## Pipeline steps\n",
    "\n",
    "1. Define schemas for `customers`, `products`, and `sales`.\n",
    "2. Ensure Bronze and exception tables exist (schema-only, no CTAS).\n",
    "3. `TRUNCATE` all Bronze and exception tables.\n",
    "4. Read CSVs in PERMISSIVE mode with `_corrupt_record`.\n",
    "5. Split clean vs malformed rows per file.\n",
    "6. Append clean rows into Bronze tables.\n",
    "7. Append malformed rows into exception tables.\n",
    "8. Print row counts for quick validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Bronze ingestion — kill and fill using TRUNCATE + exception logging\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, FloatType\n",
    ")\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Schemas\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"address1\", StringType(), True),\n",
    "    StructField(\"address2\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state_province\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"product_number\", StringType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"standard_cost\", FloatType(), True),\n",
    "    StructField(\"list_price\", FloatType(), True),\n",
    "    StructField(\"size\", StringType(), True),\n",
    "    StructField(\"weight\", FloatType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"subcategory\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", FloatType(), True),\n",
    "    StructField(\"discount\", FloatType(), True),\n",
    "    StructField(\"line_total\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Create Bronze + exception tables IF NOT EXISTS (schema only)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze_customers (\n",
    "          customer_id      string\n",
    "        , first_name       string\n",
    "        , last_name        string\n",
    "        , address1         string\n",
    "        , address2         string\n",
    "        , city             string\n",
    "        , state_province   string\n",
    "        , country          string\n",
    "        , postal_code      string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze_products (\n",
    "          product_id     string\n",
    "        , product_name   string\n",
    "        , product_number string\n",
    "        , color          string\n",
    "        , standard_cost  float\n",
    "        , list_price     float\n",
    "        , size           string\n",
    "        , weight         float\n",
    "        , category       string\n",
    "        , subcategory    string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze_sales (\n",
    "          order_id     string\n",
    "        , order_date   string\n",
    "        , customer_id  string\n",
    "        , product_id   string\n",
    "        , quantity     int\n",
    "        , unit_price   float\n",
    "        , discount     float\n",
    "        , line_total   float\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# exception tables (include _corrupt_record)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_customers (\n",
    "          customer_id      string\n",
    "        , first_name       string\n",
    "        , last_name        string\n",
    "        , address1         string\n",
    "        , address2         string\n",
    "        , city             string\n",
    "        , state_province   string\n",
    "        , country          string\n",
    "        , postal_code      string\n",
    "        , _corrupt_record  string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_products (\n",
    "          product_id       string\n",
    "        , product_name     string\n",
    "        , product_number   string\n",
    "        , color            string\n",
    "        , standard_cost    float\n",
    "        , list_price       float\n",
    "        , size             string\n",
    "        , weight           float\n",
    "        , category         string\n",
    "        , subcategory      string\n",
    "        , _corrupt_record  string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exceptions_sales (\n",
    "          order_id         string\n",
    "        , order_date       string\n",
    "        , customer_id      string\n",
    "        , product_id       string\n",
    "        , quantity         int\n",
    "        , unit_price       float\n",
    "        , discount         float\n",
    "        , line_total       float\n",
    "        , _corrupt_record  string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. TRUNCATE all Bronze + exception tables\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for tbl in [\n",
    "    \"bronze_customers\", \"exceptions_customers\",\n",
    "    \"bronze_products\", \"exceptions_products\",\n",
    "    \"bronze_sales\", \"exceptions_sales\"\n",
    "]:\n",
    "    spark.sql(f\"TRUNCATE TABLE {tbl}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Read CSVs with permissive mode and split clean vs exceptions\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def read_with_exceptions(path, schema):\n",
    "    df = (\n",
    "        spark.read\n",
    "             .schema(schema)\n",
    "             .option(\"header\", True)\n",
    "             .option(\"mode\", \"PERMISSIVE\")\n",
    "             .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "             .csv(path)\n",
    "    )\n",
    "\n",
    "    # CASE A — _corrupt_record exists\n",
    "    if \"_corrupt_record\" in df.columns:\n",
    "        clean = df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "\n",
    "        exception_schema = schema.add(\"_corrupt_record\", StringType())\n",
    "        exceptions = (\n",
    "            df.filter(col(\"_corrupt_record\").isNotNull())\n",
    "              .select([col(c) for c in exception_schema.fieldNames()])\n",
    "        )\n",
    "\n",
    "    # CASE B — no _corrupt_record column (file fully clean)\n",
    "    else:\n",
    "        clean = df\n",
    "        exception_schema = schema.add(\"_corrupt_record\", StringType())\n",
    "        exceptions = spark.createDataFrame([], exception_schema)\n",
    "\n",
    "    return clean, exceptions\n",
    "\n",
    "customers_df, exceptions_customers_df = read_with_exceptions(\"Files/customers.csv\", customers_schema)\n",
    "products_df, exceptions_products_df   = read_with_exceptions(\"Files/products.csv\", products_schema)\n",
    "sales_df, exceptions_sales_df         = read_with_exceptions(\"Files/sales.csv\", sales_schema)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Append clean + exception rows\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "customers_df.write.insertInto(\"bronze_customers\")\n",
    "products_df.write.insertInto(\"bronze_products\")\n",
    "sales_df.write.insertInto(\"bronze_sales\")\n",
    "\n",
    "exceptions_customers_df.write.insertInto(\"exceptions_customers\")\n",
    "exceptions_products_df.write.insertInto(\"exceptions_products\")\n",
    "exceptions_sales_df.write.insertInto(\"exceptions_sales\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Validate counts\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for tbl in [\n",
    "    \"bronze_customers\", \"exceptions_customers\",\n",
    "    \"bronze_products\", \"exceptions_products\",\n",
    "    \"bronze_sales\", \"exceptions_sales\"\n",
    "]:\n",
    "    print(tbl, spark.table(tbl).count())"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "217c25c8-e01f-4c1e-8dee-6198f70e8b3e",
    "default_lakehouse_name": "sales_analytics_lakehouse",
    "default_lakehouse_workspace_id": "d87dfcca-dff7-477e-ae07-42996ceb937a",
    "known_lakehouses": [
     {
      "id": "217c25c8-e01f-4c1e-8dee-6198f70e8b3e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
