{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197a4bae",
   "metadata": {},
   "source": [
    "# Bronze Ingestion — Gulf to Bay Databricks\n",
    "\n",
    "This notebook performs the Bronze‑layer ingestion for the Gulf to Bay modernization pipeline using real CSV assets stored in a Unity Catalog managed volume. The goal is to land raw operational data into Delta format with minimal transformation, preserving source fidelity while adding ingestion metadata for downstream Silver refinement.\n",
    "\n",
    "The notebook reads the raw CSV files from the `raw_files` volume, infers schema, appends an ingestion timestamp, and writes each dataset as a managed Delta table inside the `gulf_to_bay_databricks.default` schema. These Bronze tables serve as the immutable, auditable foundation for the medallion architecture and will be consumed by the Silver transformation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  BRONZE INGESTION — REAL FABRIC CSVs\n",
    "#  Gulf to Bay Databricks Modernization Pipeline\n",
    "#  Kill and fill using TRUNCATE + exception logging\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, FloatType\n",
    ")\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ============================================================\n",
    "# 1. Schemas\n",
    "# ============================================================\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"address1\", StringType(), True),\n",
    "    StructField(\"address2\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state_province\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"product_number\", StringType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"standard_cost\", FloatType(), True),\n",
    "    StructField(\"list_price\", FloatType(), True),\n",
    "    StructField(\"size\", StringType(), True),\n",
    "    StructField(\"weight\", FloatType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"subcategory\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", FloatType(), True),\n",
    "    StructField(\"discount\", FloatType(), True),\n",
    "    StructField(\"line_total\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# 2. Create Bronze + exception tables IF NOT EXISTS\n",
    "# ============================================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gulf_to_bay_databricks.default.bronze_customers (\n",
    "          customer_id      string\n",
    "        , first_name       string\n",
    "        , last_name        string\n",
    "        , address1         string\n",
    "        , address2         string\n",
    "        , city             string\n",
    "        , state_province   string\n",
    "        , country          string\n",
    "        , postal_code      string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gulf_to_bay_databricks.default.bronze_products (\n",
    "          product_id     string\n",
    "        , product_name   string\n",
    "        , product_number string\n",
    "        , color          string\n",
    "        , standard_cost  float\n",
    "        , list_price     float\n",
    "        , size           string\n",
    "        , weight         float\n",
    "        , category       string\n",
    "        , subcategory    string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gulf_to_bay_databricks.default.bronze_sales (\n",
    "          order_id     string\n",
    "        , order_date   string\n",
    "        , customer_id  string\n",
    "        , product_id   string\n",
    "        , quantity     int\n",
    "        , unit_price   float\n",
    "        , discount     float\n",
    "        , line_total   float\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Exception tables (include _corrupt_record)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gulf_to_bay_databricks.default.exceptions_customers (\n",
    "          customer_id      string\n",
    "        , first_name       string\n",
    "        , last_name        string\n",
    "        , address1         string\n",
    "        , address2         string\n",
    "        , city             string\n",
    "        , state_province   string\n",
    "        , country          string\n",
    "        , postal_code      string\n",
    "        , _corrupt_record  string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gulf_to_bay_databricks.default.exceptions_products (\n",
    "          product_id       string\n",
    "        , product_name     string\n",
    "        , product_number   string\n",
    "        , color            string\n",
    "        , standard_cost    float\n",
    "        , list_price       float\n",
    "        , size             string\n",
    "        , weight           float\n",
    "        , category         string\n",
    "        , subcategory      string\n",
    "        , _corrupt_record  string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gulf_to_bay_databricks.default.exceptions_sales (\n",
    "          order_id         string\n",
    "        , order_date       string\n",
    "        , customer_id      string\n",
    "        , product_id       string\n",
    "        , quantity         int\n",
    "        , unit_price       float\n",
    "        , discount         float\n",
    "        , line_total       float\n",
    "        , _corrupt_record  string\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. TRUNCATE all Bronze + exception tables\n",
    "# ============================================================\n",
    "\n",
    "for tbl in [\n",
    "    \"bronze_customers\", \"exceptions_customers\",\n",
    "    \"bronze_products\", \"exceptions_products\",\n",
    "    \"bronze_sales\", \"exceptions_sales\"\n",
    "]:\n",
    "    spark.sql(f\"TRUNCATE TABLE gulf_to_bay_databricks.default.{tbl}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. Read CSVs with permissive mode and split clean vs exceptions\n",
    "# ============================================================\n",
    "\n",
    "def read_with_exceptions(path, schema):\n",
    "    df = (\n",
    "        spark.read\n",
    "             .schema(schema)\n",
    "             .option(\"header\", True)\n",
    "             .option(\"mode\", \"PERMISSIVE\")\n",
    "             .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "             .csv(path)\n",
    "    )\n",
    "\n",
    "    # CASE A — _corrupt_record exists\n",
    "    if \"_corrupt_record\" in df.columns:\n",
    "        clean = df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "\n",
    "        exception_schema = schema.add(\"_corrupt_record\", StringType())\n",
    "        exceptions = (\n",
    "            df.filter(col(\"_corrupt_record\").isNotNull())\n",
    "              .select([col(c) for c in exception_schema.fieldNames()])\n",
    "        )\n",
    "\n",
    "    # CASE B — no _corrupt_record column (file fully clean)\n",
    "    else:\n",
    "        clean = df\n",
    "        exception_schema = schema.add(\"_corrupt_record\", StringType())\n",
    "        exceptions = spark.createDataFrame([], exception_schema)\n",
    "\n",
    "    return clean, exceptions\n",
    "\n",
    "VOLUME_BASE = \"/Volumes/gulf_to_bay_databricks/default/raw_files\"\n",
    "\n",
    "customers_df, exceptions_customers_df = read_with_exceptions(f\"{VOLUME_BASE}/customers.csv\", customers_schema)\n",
    "products_df, exceptions_products_df   = read_with_exceptions(f\"{VOLUME_BASE}/products.csv\", products_schema)\n",
    "sales_df, exceptions_sales_df         = read_with_exceptions(f\"{VOLUME_BASE}/sales.csv\", sales_schema)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Append clean + exception rows\n",
    "# ============================================================\n",
    "\n",
    "customers_df.write.insertInto(\"gulf_to_bay_databricks.default.bronze_customers\")\n",
    "products_df.write.insertInto(\"gulf_to_bay_databricks.default.bronze_products\")\n",
    "sales_df.write.insertInto(\"gulf_to_bay_databricks.default.bronze_sales\")\n",
    "\n",
    "exceptions_customers_df.write.insertInto(\"gulf_to_bay_databricks.default.exceptions_customers\")\n",
    "exceptions_products_df.write.insertInto(\"gulf_to_bay_databricks.default.exceptions_products\")\n",
    "exceptions_sales_df.write.insertInto(\"gulf_to_bay_databricks.default.exceptions_sales\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Validate counts\n",
    "# ============================================================\n",
    "\n",
    "for tbl in [\n",
    "    \"bronze_customers\", \"exceptions_customers\",\n",
    "    \"bronze_products\", \"exceptions_products\",\n",
    "    \"bronze_sales\", \"exceptions_sales\"\n",
    "]:\n",
    "    count = spark.table(f\"gulf_to_bay_databricks.default.{tbl}\").count()\n",
    "    print(f\"{tbl}: {count}\")\n",
    "\n",
    "print(\"\\n✅ Bronze ingestion complete!\")\n",
    "display(spark.table(\"gulf_to_bay_databricks.default.bronze_sales\").limit(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
